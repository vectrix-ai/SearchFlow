{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Page Indexing and Vectorization ðŸ‘€\n",
    "\n",
    "This Jupyter notebook contains a script that performs indexing and vectorization of web page contents. The primary purpose of this script is to crawl through a specified web page, extract the textual contents, and subsequently store these contents as vector objects in a database.\n",
    "\n",
    "The vectorized information can then be utilized in a Retrieval-Augmented Generation (RAG) flow to answer questions using a Language Model (LLM). This process enables the creation of a more context-aware and responsive system, capable of providing detailed responses based on the indexed and vectorized information from the web page.\n",
    "\n",
    "The notebook is structured in a step-by-step manner, guiding you through the process of web page crawling, text extraction, vectorization, and storage in a database. Each step is accompanied by detailed explanations and code snippets to provide a comprehensive understanding of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "from trafilatura import extract\n",
    "import validators, json\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crawler and Content Extractor\n",
    "\n",
    "This code implements a web crawler and content extractor that:\n",
    "\n",
    "1. Extracts URLs from the given HTML content, filtering for the same domain and validating the URLs. âœ…\n",
    "2. Crawls a website starting from a given URL, iteratively processing and extracting links from each page. âœ…\n",
    "3. Returns a mist of HTML documents extracted from the website âœ…\n",
    "\n",
    "The code displays the source URL of each processed page and the total number of pages in the extracted content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the site name\n",
    "url = \"https://www.loopearplugs.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extraction pipeline for site:  vectrix.ai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/GitHub/paginx/.venv/lib/python3.12/site-packages/langchain_community/document_loaders/async_html.py:193: UserWarning: For better logging of progress, `pip install tqdm`\n",
      "  warnings.warn(\"For better logging of progress, `pip install tqdm`\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting the following links:  ['http://www.vectrix.ai/projects/products', 'http://www.vectrix.ai/projects/projects', 'https://www.vectrix.ai/Services', 'https://www.vectrix.ai/platform', 'https://www.vectrix.ai/contact-us', 'https://www.vectrix.ai/career', 'https://www.vectrix.ai/blog', 'http://www.vectrix.ai/projects/advice', 'https://www.vectrix.ai/about-us']\n",
      "Number of pages processed:  9\n",
      "Number of links to visit:  6\n",
      "Visiting the following links:  ['https://www.vectrix.ai/projects/products', 'https://www.vectrix.ai/projects/projects', 'https://www.vectrix.ai/projects/advice', 'https://www.vectrix.ai/job-list/junior-ai-researcher', 'https://www.vectrix.ai/job-list/software-engineer-front-end', 'https://www.vectrix.ai/job-list/internship']\n",
      "Number of pages processed:  15\n",
      "Number of links to visit:  0\n",
      "Download finished. Extracting content from the pages.\n",
      "Number of pages scraped:  16\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "from bs4 import BeautifulSoup\n",
    "import validators\n",
    "from trafilatura import extract\n",
    "\n",
    "def is_valid_url(url_string: str) -> bool:\n",
    "    result = validators.url(url_string)\n",
    "    # Url with the words Slide-template are not valid\n",
    "    if \"slide-template\" in url_string.lower():\n",
    "        return False\n",
    "    if result:\n",
    "        return result\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def prepend_url(base_url, link):\n",
    "    if link.startswith('/'):\n",
    "        return base_url + link\n",
    "    else:\n",
    "        return link\n",
    "    \n",
    "def strip_query_string(url):\n",
    "    parsed = urlparse(url)\n",
    "    return parsed.scheme + \"://\" + parsed.netloc + parsed.path\n",
    "\n",
    "def extract_site_urls(html: str, site_name:str, url: str) -> list:\n",
    "    '''\n",
    "    Extract the URLs from the HTML content\n",
    "    Only if the domain name is the same\n",
    "    Returns a list of strings\n",
    "    '''\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = [link.get(\"href\") for link in soup.find_all(\"a\") if link.get(\"href\") is not None]\n",
    "\n",
    "    # Remove empty links and mailto links\n",
    "    links = [link for link in links if len(link) > 1 and not link.startswith(\"mailto\")]\n",
    "    \n",
    "    # Add the base URL to the links\n",
    "    links = [prepend_url(url, link) for link in links]\n",
    "    links = [url + link if not link.startswith(\"http\") else link for link in links]\n",
    "    # Remove links that are not from the same site\n",
    "    links = [link for link in links if site_name in link]\n",
    "    # Check fo valid URLs\n",
    "    links = [link for link in links if is_valid_url(link)]\n",
    "    # Remove everything after the # sign\n",
    "    links = [link.split(\"#\")[0] for link in links]\n",
    "    # Remove duplicates\n",
    "    links = list(set(links))    \n",
    "    return links    \n",
    "\n",
    "\n",
    "def get_site_contents(url:str, max_pages:int = 200, check_query_strings: bool = False) -> list:\n",
    "    '''\n",
    "    The input as a URL\n",
    "    Returns the site content as Markdown and a list of links (from that same site)\n",
    "    '''\n",
    "    site_name = url.split(\"//\")[1].split(\"/\")[0].replace(\"www.\", \"\")\n",
    "    print(\"Starting extraction pipeline for site: \", site_name)\n",
    "\n",
    "    visited_links = []\n",
    "\n",
    "    loader = AsyncHtmlLoader(url)\n",
    "    index_page = loader.load()\n",
    "    visited_links.append(url)\n",
    "\n",
    "    html = index_page[0].page_content\n",
    "\n",
    "\n",
    "    links = extract_site_urls(html, site_name, url)\n",
    "    processed_pages = []\n",
    "\n",
    "\n",
    "    while len(links) > 0 :\n",
    "        print(\"Visiting the following links: \", links)\n",
    "        other_pages = AsyncHtmlLoader(links, ignore_load_errors=True)\n",
    "        docs = other_pages.load()\n",
    "        processed_pages.extend(docs)\n",
    "        if len(processed_pages) > max_pages:\n",
    "            break\n",
    "        visited_links.extend(strip_query_string(link) for link in links)\n",
    "        print(\"Number of pages processed: \", len(processed_pages))\n",
    "        for doc in docs:\n",
    "            # Extracting links from \n",
    "            links.extend(extract_site_urls(doc.page_content, site_name, url))\n",
    "\n",
    "        # Remove visited links\n",
    "        links = [link for link in links if strip_query_string(link) not in visited_links]\n",
    "        print(\"Number of links to visit: \", len(links))\n",
    "\n",
    "    processed_pages.extend(index_page)\n",
    "\n",
    "    print(\"Download finished. Extracting content from the pages.\")\n",
    "    # Apply the excract method to each element of the list\n",
    "    docs_transformed = [extract(doc.page_content, output_format=\"json\", include_comments=False) for doc in processed_pages]\n",
    "    \n",
    "    #html2text = Html2TextTransformer()\n",
    "    #docs_transformed = html2text.transform_documents(processed_pages)\n",
    "\n",
    "    return docs_transformed\n",
    "\n",
    "scraped_pages = get_site_contents(url, 1000)\n",
    "print(\"Number of pages scraped: \", len(scraped_pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Chunking\n",
    "In this step we will split all the extracted web pages into logical chunks. \n",
    "\n",
    "âž¡ï¸ We will use the [trafilatura](https://trafilatura.readthedocs.io/en/latest/) library to extract the main content of the web pages. It will return a json objects with the following attributes\n",
    "- `title`: The title of the page\n",
    "- `author`: The author of the page, in most cases this will be empty\n",
    "- `hostname`: The hostname of the page\n",
    "- `date`: The date of the page\n",
    "- `fingerprint`: A fingerprint of the page\n",
    "- `id`: The id of the page, most of the time this will be empty\n",
    "- `license`: The license of the page, most of the time this will be empty\n",
    "- `comments`: The comments of the page, most of the time this will be empty\n",
    "- `raw_text` : The raw text of the page: html elements are removed, also visual elements are removed\n",
    "- `language`: The language of the page\n",
    "- `image`: The images of the page, contains the URLs\n",
    "- `pagetype`: Always set to website\n",
    "- `source`: Main URL of the website\n",
    "- `source-hostname`: Hostname of the website\n",
    "- `excerpt`: An excerpt of the page\n",
    "- `categories`: The categories of the page\n",
    "- `tags`: The tags of the page\n",
    "\n",
    "âž¡ï¸ We will pipe this to another splitter to further cut the sections into smaller chunks if they are too large. For this we use Langchains \n",
    "\n",
    "âž¡ï¸  Also we will attach an LLM to the chain to ignore chunks that are not relevant, for example: navigation bars, footers, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking and metadata extraction\n",
    "Using the functions below we extract the medata and devide the text into chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def extract_metadata(pages: list) -> list:\n",
    "    '''\n",
    "    This function will extract the metdata extracted from the pages by the Trafilatura library\n",
    "    '''\n",
    "    keys =  ['title', 'hostname', 'image', 'source', 'source-hostname', 'excerpt']\n",
    "    metadata = []\n",
    "\n",
    "    for page in pages:\n",
    "        page = json.loads(page)\n",
    "        metadata.append({key: page[key] for key in keys if key in page})\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def ner_processing(content: list, metadatas: list, chunk_size: int = 500) -> list:\n",
    "    '''\n",
    "    Split the content into chunks of a certain size;\n",
    "    Inputs:\n",
    "    - content: list of strings\n",
    "    - metadatas: list of dictionaries\n",
    "    - chunk_size: int (optional), default 1000\n",
    "\n",
    "    Returns a list of dictionaries\n",
    "    '''\n",
    "\n",
    "    # Splitting the content into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\",\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "    return text_splitter.create_documents(content, metadatas=metadatas)\n",
    "\n",
    "\n",
    "\n",
    "metadata = extract_metadata(scraped_pages)\n",
    "content = [' '.join(json.loads(page)[\"raw_text\"].split()) for page in scraped_pages]\n",
    "chunks = ner_processing(content, metadata)\n",
    "#chunks = [chunk.dict() for chunk in chunks]\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Vectrix - Projects', 'hostname': None, 'image': None, 'source': None, 'source-hostname': None, 'excerpt': 'Vectrix specializes in using generative AI to streamline business operations, simplify complex processes, automate routine tasks, and create easy-to-integrate, intuitive solutions.'}\n"
     ]
    }
   ],
   "source": [
    "print(chunks[2].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER Extraction Pipeline\n",
    "Here we will use langchain and and LLM to extract the Named Entities from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import Replicate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "#llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\")\n",
    "#llm = Ollama(model_name=\"llama3-70b-8192\", temperature=0)\n",
    "\n",
    "llm = Replicate(\n",
    "    model=\"meta/meta-llama-3-70b-instruct\",\n",
    "    model_kwargs={\"temperature\": 0},\n",
    ")\n",
    "\n",
    "#llm = ChatAnthropic(model='claude-3-sonnet-20240229')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    entity_type: Optional[str] = Field(description=\"The type of the entity, for example 'person', 'location', 'organization' etc.\")\n",
    "    entity_name: Optional[str] = Field(description=\"The name of the entity, for example 'John Doe', 'New York', 'Apple Inc.' etc.\")\n",
    "\n",
    "# Define your desired data structure.\n",
    "class NERExtraction(BaseModel):\n",
    "    entity_list: List[Entity] = Field(description=\"List of entities extracted from the text\")\n",
    "    language: str = Field(description=\"The language of the text\")\n",
    "    category: str = Field(description=\"Return the subject what this text excaclty is about\")\n",
    "\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=NERExtraction)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"entity_list\": [\n",
      "    {\n",
      "      \"entity_type\": \"person\",\n",
      "      \"entity_name\": \"Alex\"\n",
      "    },\n",
      "    {\n",
      "      \"entity_type\": \"project\",\n",
      "      \"entity_name\": \"Services Project\"\n",
      "    }\n",
      "  ],\n",
      "  \"language\": \"English\",\n",
      "  \"category\": \"Personal Experience\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"query\": chunks[2].page_content})\n",
    "print(response.json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 of 16 completed.\n",
      "Task 2 of 16 completed.\n",
      "Task 3 of 16 completed.\n",
      "Task 4 of 16 completed.\n",
      "Task 5 of 16 completed.\n",
      "Task 6 of 16 completed.\n",
      "Task 7 of 16 completed.\n",
      "Task 8 of 16 completed.\n",
      "Task 9 of 16 completed.\n",
      "Task 10 of 16 completed.\n",
      "Task 11 of 16 completed.\n",
      "Task 12 of 16 completed.\n",
      "Task 13 of 16 completed.\n",
      "Task 14 of 16 completed.\n",
      "Task 15 of 16 completed.\n",
      "Task 16 of 16 completed.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def process_page_content(chunk, semaphore):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            response = await chain.ainvoke({\"query\": chunk.page_content})\n",
    "            chunk.metadata['NER'] = response\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None  # or some other value indicating failure\n",
    "\n",
    "async def main():\n",
    "    semaphore = asyncio.Semaphore(4)  # Limit concurrency to 4\n",
    "    tasks = []\n",
    "    chunks_with_responses = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        task = asyncio.create_task(process_page_content(chunk, semaphore))\n",
    "        tasks.append(task)\n",
    "    \n",
    "    responses = []\n",
    "    for i, future in enumerate(asyncio.as_completed(tasks)):\n",
    "        chunk = await future\n",
    "        chunks_with_responses.append(chunk)\n",
    "        print(f\"Task {i+1} of {len(tasks)} completed.\")  # Print a message here\n",
    "    \n",
    "    return chunks_with_responses\n",
    "\n",
    "# Get the current event loop\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "# Run the main function using the current event loop\n",
    "results = loop.run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_content': 'At Vectrix, we specialize in using advanced technology, particularly generative AI, to transform the way businesses operate. Our focus is on simplifying complex processes, automating routine tasks, and designing unique, intuitive solutions that are easy to integrate and use.', 'metadata': {'title': 'Blog', 'hostname': None, 'image': None, 'source': None, 'source-hostname': None, 'excerpt': 'Create an impactful agency website with Baseline. Featuring a sleek and minimal visual design, enterprise-grade development, and seamless animations. Flowgency offers effortless customization, lightning-fast performance, and full responsiveness across all devices.', 'NER': {'entity_list': [{'entity_type': 'organization', 'entity_name': 'Vectrix'}], 'language': 'English', 'category': 'Business Technology'}}, 'type': 'Document'}\n"
     ]
    }
   ],
   "source": [
    "print(results[5].dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used:  195.75 MB\n"
     ]
    }
   ],
   "source": [
    "# Show the memory usage of this notebook\n",
    "import os\n",
    "import psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Memory used: \", process.memory_info().rss / 1024 ** 2, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the results in a postgres database"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Pull the postgres image, install the pgvector extension and run the container\n",
    "!docker pull ankane/pgvector\n",
    "!docker run -d --name paginx -e POSTGRES_PASSWORD=mysecretpassword -p 5432:5432 -e PG_EXTENSIONS=\"pgvector\" ankane/pgvector"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Connect to the default database (usually 'postgres')\n",
    "default_engine = create_engine('postgresql://postgres:mysecretpassword@localhost/postgres')\n",
    "\n",
    "# Create a new database named paginx if it doesn't exist\n",
    "with default_engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT 1 FROM pg_database WHERE datname = 'paginx'\"))\n",
    "    if not result.scalar():\n",
    "        connection.execute(text(\"COMMIT\"))  # Commit any open transactions\n",
    "        connection.execute(text(\"CREATE DATABASE paginx\"))\n",
    "\n",
    "# Connect to the newly created database\n",
    "paginx_engine = create_engine('postgresql://postgres:mysecretpassword@localhost/paginx')\n",
    "\n",
    "# Install the pgvector extension in the paginx database\n",
    "with paginx_engine.connect() as connection:\n",
    "    connection.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "connection = \"postgresql+psycopg://postgres:mysecretpassword@localhost/paginx\"\n",
    "collection_name = url\n",
    "embeddings = CohereEmbeddings()\n",
    "\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "vectorstore.add_documents(chunks, ids=[chunk.metadata[\"id\"] for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorstore.similarity_search(\"Ben\", k=3)[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
