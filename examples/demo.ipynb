{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", ResourceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SearchFlow Demo ðŸ‘¨ðŸ»â€ðŸ’»\n",
    "This notebook demonstrates the functions for importing data from various sources. \n",
    "Loading it into a VectorStore, and then using it to answer questions with a Retrieval Augemented Reasoning  ðŸ¦œðŸ”— LangGraph.\n",
    "\n",
    "## Creating a new project\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Test'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from searchflow.db import DB\n",
    "db = DB()\n",
    "\n",
    "print(db.list_projects())\n",
    "db.create_project(\"Test\", description=\"This is a test project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.remove_project(\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "### 1. From a URL ðŸ”—\n",
    "\n",
    "**Web Crawling and Data Extraction Example**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchflow.importers import WebScraper\n",
    "\n",
    "scraper = WebScraper(project_name='Test', db=db)\n",
    "scraper.get_all_links(\"https://vectrix.ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchflow.importers import WebScraper\n",
    "\n",
    "scraper = WebScraper(project_name='Test', db=db)\n",
    "scraper.full_import(\"https://dataframe.be\", max_pages=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db.get_links_to_confirm(\"Test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.add_links_to_index(links=[\"https://langchain-ai.github.io/langgraph/reference/graphs/\"],base_url=\"https://langchain-ai.github.io/langgraph/reference/graphs/\", project_name=\"Test\", status=\"Confirm page import\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db.get_indexing_status(\"Test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchflow.importers import WebScraper\n",
    "\n",
    "scraper = WebScraper(project_name='Test', db=db)\n",
    "\n",
    "confirmed_links = [link['url'] for link in db.get_links_to_confirm(\"Test\")]\n",
    "scraper.download_pages(confirmed_links, project_name=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Upload files â¬†ï¸\n",
    "You can also upload files and add them to the vector store, Vectrix will automaticly detect the file type extract the text and chunk the content into blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-03 17:52:34,360 - Files - INFO - Processing files locally\u001b[0m\n",
      "\u001b[32m2024-09-03 17:52:34,909 - Files - INFO - Processing file 1 of 1\u001b[0m\n",
      "\u001b[32m2024-09-03 17:52:34,960 - Files - INFO - Uploaded test.pdf to object storage\u001b[0m\n",
      "\u001b[32m2024-09-03 17:53:00,509 - Files - INFO - Chunked the document into 1 parts\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from searchflow.importers import Files\n",
    "file = './files/pdf_with_scannedtext.pdf'\n",
    "\n",
    "# Load bytes data\n",
    "with open(file, \"rb\") as f:\n",
    "    bytes_data = f.read()\n",
    "\n",
    "files = Files()\n",
    "files.upload_file(\n",
    "    document_data=[(bytes_data, \"test.pdf\")],\n",
    "    project_name=\"Test\",\n",
    "    inference_type=\"local\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove a file\n",
    "db.remove_file(\"Test\", \"test.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Chrome Plugin ðŸ¦Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the FastAPI server\n",
    "#!python src/searchflow/api.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT COUNT(*) AS total_webpages FROM document_metadata WHERE file_type = 'webpage';\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import tool\n",
    "from searchflow.db import DB\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "\n",
    "\n",
    "llm = llm.bind_tools([QueryResult], tool_choice=\"required\", strict=True)\n",
    "\n",
    "@tool\n",
    "def run_sql_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Run a SQL query on the database.\n",
    "    \"\"\"\n",
    "    db = DB()\n",
    "    result = db.run_query(query)\n",
    "    return result\n",
    "\n",
    "\n",
    "prompt = hub.pull(\"write_query\")\n",
    "\n",
    "chain = prompt | llm \n",
    "\n",
    "response = chain.invoke({\"USER_QUESTION\": \"How many webpages are there?\"}).tool_calls\n",
    "print(response[0]['args']['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(47,)]\n"
     ]
    }
   ],
   "source": [
    "answer = run_sql_query(response[0]['args']['query'])\n",
    "print(answer.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Database Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from searchflow.db import DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DB()\n",
    "db = SQLDatabase(db.engine)\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True, use_query_checker=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "How many blog posts are writteb by Dimitri Allaert ?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT COUNT(*) \n",
      "FROM document_metadata \n",
      "WHERE author = 'Dimitri Allaert' AND file_type = 'blog_post';\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(0,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3m0\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain.run(\"How many blog posts are writteb by Dimitri Allaert ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\n",
    "Use the following format:\n",
    "\n",
    "Question: \"Question here\"\n",
    "SQLQuery: \"SQL Query to run\"\n",
    "SQLResult: \"Result of the SQLQuery\"\n",
    "Answer: \"Final answer here\"\n",
    "\n",
    "Only use the following table:\n",
    "DDL:\n",
    "CREATE TABLE document_metadata (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    title VARCHAR(255) NOT NULL,\n",
    "    author VARCHAR(255),\n",
    "    file_type VARCHAR(50),\n",
    "    word_count INTEGER,\n",
    "    language VARCHAR(50),\n",
    "    source VARCHAR(255),\n",
    "    content_type VARCHAR(100),\n",
    "    tags TEXT[],  -- Array of strings\n",
    "    summary TEXT,\n",
    "    url VARCHAR(255),\n",
    "    project_name VARCHAR(255),\n",
    "    indexing_status VARCHAR(50),\n",
    "    filename VARCHAR(255),\n",
    "    priority INTEGER,\n",
    "    read_time FLOAT,  -- In minutes\n",
    "    creation_date TIMESTAMPTZ,\n",
    "    last_modified_date TIMESTAMPTZ,\n",
    "    upload_date TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,\n",
    "    CONSTRAINT uq_doc_url_project UNIQUE (url, project_name),\n",
    "    CONSTRAINT fk_project_name FOREIGN KEY (project_name) REFERENCES projects(name)\n",
    ");\n",
    "\n",
    "Content:\n",
    "id,title,author,file_type,word_count,language,source,content_type,tags,summary,url,project_name,indexing_status,filename,priority,read_time,creation_date,last_modified_date,upload_date\n",
    "8,AI Expertisecentrum,,webpage,408,NL,chrome_extension,blog_post,\"[\"\"AI\"\",\"\"Vlaamse overheid\"\",\"\"digitale transformatie\"\",\"\"ethiek\"\",\"\"kennisdeling\"\"]\",Het AI Expertisecentrum ondersteunt de Vlaamse overheid en lokale overheden in het gebruik van artificiÃ«le intelligentie om de efficiÃ«ntie en innovatie van diensten te verbeteren. Het centrum biedt een kader voor AI-toepassingen en stimuleert kennisdeling binnen de overheid.,https://www.vlaanderen.be/digitaal-vlaanderen/onze-oplossingen/ai-expertisecentrum,Test,,AI Expertisecentrum,,2.04,,,2024-09-03 16:27:05.904255+00\n",
    "9,Wat betekent de aankomende NIS2-richtlijn (cybersecurity wetgeving)â€¦,,webpage,889,NL,chrome_extension,blog_post,\"[\"\"Cybersecurity\"\",\"\"NIS2\"\",\"\"GDPR\"\",\"\"Belgium\"\",\"\"Government\"\",\"\"Digital Security\"\"]\",\"The NIS2 directive, effective from October 2024 in Belgium, aims to enhance cybersecurity across Europe, impacting government leaders by imposing strict security standards similar to GDPR. It emphasizes the responsibility of organizations in managing digital security, requiring comprehensive risk management, incident response plans, and a culture of security awareness among employees.\",https://www.vlaanderen.be/digitaal-vlaanderen/wat-betekent-de-aankomende-nis2-richtlijn-cybersecurity-wetgeving-voor-leidinggevenden-bij-de-overheid,Vlaamse Overheid,,Wat betekent de aankomende NIS2-richtlijn (cybersecurity wetgeving)â€¦,,4.445,,,2024-09-03 16:30:03.524486+00\n",
    "10,Vectrix Mail,,webpage,5070,EN,chrome_extension,email,\"[\"\"AI\"\",\"\"Community\"\",\"\"Event\"\",\"\"Newsletter\"\",\"\"Invitation\"\"]\",\"The content consists of various email communications regarding community events, invitations, confirmations, and newsletters related to AI and business activities. Key participants include Paulien Derden and Dimitri Allaert, with discussions about supporting an AI community in Antwerp, event invitations, and confirmations for conferences and meetings.\",https://mail.google.com/mail/u/0/#search/bart/FMfcgzQVzNvVshgTnGNKFwPdtwZrsXNt,Vlaamse Overheid,,Vectrix Mail,,25.35,,,2024-09-03 16:32:39.998075+00\n",
    "11,Vectrix - SLM Training & AI Solutions,,webpage,126,EN,webpage,other,\"[\"\"AI\"\",\"\"Technology\"\",\"\"Business\"\",\"\"Product Development\"\"]\",\"The content discusses an AI solution development process, including initial discussions, feasibility checks, MVP development, and final product creation, emphasizing security, compliance, and user control.\",https://vectrix.ai/,Vectrix,,,,0.63,,,2024-09-03 17:00:00.472415+00\n",
    "\n",
    "\n",
    "If someone asks for the table foobar, they really mean the employee table.\n",
    "\n",
    "Question: {input}\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"input\", \"dialect\"], template=_DEFAULT_TEMPLATE\n",
    ")\n",
    "\n",
    "db_chain = SQLDatabaseChain.from_llm(llm, db, prompt=PROMPT, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "What are all the various sources of content in the data ?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT DISTINCT source FROM document_metadata;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[('chrome_extension',), ('webpage',), ('uploaded_file',)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThe various sources of content in the data are: chrome_extension, webpage, and uploaded_file.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The various sources of content in the data are: chrome_extension, webpage, and uploaded_file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(db_chain.run(\"What are all the various sources of content in the data ?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
