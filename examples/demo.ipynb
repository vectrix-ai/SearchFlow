{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", ResourceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectrix Demo üë®üèª‚Äçüíª\n",
    "This notebook demonstrates the functions for importing data from various sources. \n",
    "Loading it into a VectorStore, and then using it to answer questions with a Retrieval Augemented Reasoning  ü¶úüîó LangGraph.\n",
    "\n",
    "## Creating a new project\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from vectrix.db import DB\n",
    "db = DB()\n",
    "print(db.list_projects())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db.get_collection_metdata(\"Vectrix\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.create_project(\"Vectrix\", description=\"Demo project for Vectrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.remove_project(\"Vectrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "### 1. From a URL üîó\n",
    "\n",
    "**Web Crawling and Data Extraction Example**<>\n",
    "\n",
    "This cell demonstrates the process of crawling a website, chunking the extracted content, and performing Named Entity Recognition (NER) using the Vectrix library.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Web Crawling**: \n",
    "   - Uses `vectrix.Crawler` to extract pages from \"https://vectrix.ai\"\n",
    "   - Limits the crawl to a maximum of 20 pages\n",
    "\n",
    "2. **Content Chunking**:\n",
    "   - Utilizes `vectrix.Webchunker` to break down the extracted content\n",
    "   - Chunks are created with a size of 500 characters and an overlap of 50 characters\n",
    "\n",
    "3. **Named Entity Recognition**:\n",
    "   - Employs `vectrix.Extract` with the 'Replicate' model\n",
    "   - Uses the 'meta/meta-llama-3-70b-instruct' model for entity extraction\n",
    "\n",
    "This example showcases a typical workflow for web data extraction and processing using the Vectrix library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set/updated scrape status for base URL: https://www.vectrix.ai/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2024-08-20 16:46:56,737 - trafilatura.downloads - ERROR - not a 200 response: 404 for URL https://www.vectrix.ai/robots.txt\u001b[0m\n",
      "\u001b[31m2024-08-20 16:46:56,737 - trafilatura.downloads - ERROR - not a 200 response: 404 for URL https://www.vectrix.ai/robots.txt\u001b[0m\n",
      "\u001b[31m2024-08-20 16:46:56,737 - trafilatura.downloads - ERROR - not a 200 response: 404 for URL https://www.vectrix.ai/robots.txt\u001b[0m\n",
      "\u001b[31m2024-08-20 16:46:57,156 - trafilatura.downloads - ERROR - not a 200 response: 404 for URL https://www.vectrix.ai/robots.txt\u001b[0m\n",
      "\u001b[31m2024-08-20 16:46:57,156 - trafilatura.downloads - ERROR - not a 200 response: 404 for URL https://www.vectrix.ai/robots.txt\u001b[0m\n",
      "\u001b[31m2024-08-20 16:46:57,156 - trafilatura.downloads - ERROR - not a 200 response: 404 for URL https://www.vectrix.ai/robots.txt\u001b[0m\n",
      "\u001b[33m2024-08-20 16:46:57,268 - courlan.urlstore - WARNING - Discarding URL: /blog-post/understanding-large-and-small-language-models-key-differences-and-applications\u001b[0m\n",
      "\u001b[33m2024-08-20 16:46:57,268 - courlan.urlstore - WARNING - Discarding URL: /blog-post/understanding-large-and-small-language-models-key-differences-and-applications\u001b[0m\n",
      "\u001b[33m2024-08-20 16:46:57,268 - courlan.urlstore - WARNING - Discarding URL: /blog-post/understanding-large-and-small-language-models-key-differences-and-applications\u001b[0m\n",
      "\u001b[33m2024-08-20 16:46:58,456 - courlan.urlstore - WARNING - Discarding URL: /platform\u001b[0m\n",
      "\u001b[33m2024-08-20 16:46:58,456 - courlan.urlstore - WARNING - Discarding URL: /platform\u001b[0m\n",
      "\u001b[33m2024-08-20 16:46:58,456 - courlan.urlstore - WARNING - Discarding URL: /platform\u001b[0m\n",
      "\u001b[33m2024-08-20 16:46:59,650 - courlan.urlstore - WARNING - Discarding URL: /blog\u001b[0m\n",
      "\u001b[33m2024-08-20 16:46:59,650 - courlan.urlstore - WARNING - Discarding URL: /blog\u001b[0m\n",
      "\u001b[33m2024-08-20 16:46:59,650 - courlan.urlstore - WARNING - Discarding URL: /blog\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:00,838 - courlan.urlstore - WARNING - Discarding URL: /blog-post/are-llm-benchmarks-and-leaderboards-just-marketing-tools\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:00,838 - courlan.urlstore - WARNING - Discarding URL: /blog-post/are-llm-benchmarks-and-leaderboards-just-marketing-tools\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:00,838 - courlan.urlstore - WARNING - Discarding URL: /blog-post/are-llm-benchmarks-and-leaderboards-just-marketing-tools\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:02,002 - courlan.urlstore - WARNING - Discarding URL: /blog-post/your-ai-might-be-misleading-you-understanding-the-dual-nature-of-llm-outputs\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:02,002 - courlan.urlstore - WARNING - Discarding URL: /blog-post/your-ai-might-be-misleading-you-understanding-the-dual-nature-of-llm-outputs\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:02,002 - courlan.urlstore - WARNING - Discarding URL: /blog-post/your-ai-might-be-misleading-you-understanding-the-dual-nature-of-llm-outputs\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:03,141 - courlan.urlstore - WARNING - Discarding URL: /career\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:03,141 - courlan.urlstore - WARNING - Discarding URL: /career\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:03,141 - courlan.urlstore - WARNING - Discarding URL: /career\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:04,283 - courlan.urlstore - WARNING - Discarding URL: /contact-us\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:04,283 - courlan.urlstore - WARNING - Discarding URL: /contact-us\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:04,283 - courlan.urlstore - WARNING - Discarding URL: /contact-us\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:05,454 - courlan.urlstore - WARNING - Discarding URL: /about-us\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:05,454 - courlan.urlstore - WARNING - Discarding URL: /about-us\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:05,454 - courlan.urlstore - WARNING - Discarding URL: /about-us\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:06,638 - courlan.urlstore - WARNING - Discarding URL: /blog-post/google-deepminds-searchless-chess-engine---part-1\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:06,638 - courlan.urlstore - WARNING - Discarding URL: /blog-post/google-deepminds-searchless-chess-engine---part-1\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:06,638 - courlan.urlstore - WARNING - Discarding URL: /blog-post/google-deepminds-searchless-chess-engine---part-1\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:07,796 - courlan.urlstore - WARNING - Discarding URL: /offerings\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:07,796 - courlan.urlstore - WARNING - Discarding URL: /offerings\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:07,796 - courlan.urlstore - WARNING - Discarding URL: /offerings\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:08,965 - courlan.urlstore - WARNING - Discarding URL: /blog-post/advanced-applications-and-future-trends-in-entity-analysis\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:08,965 - courlan.urlstore - WARNING - Discarding URL: /blog-post/advanced-applications-and-future-trends-in-entity-analysis\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:08,965 - courlan.urlstore - WARNING - Discarding URL: /blog-post/advanced-applications-and-future-trends-in-entity-analysis\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:10,154 - courlan.urlstore - WARNING - Discarding URL: /blog-post/image-extraction-with-langchain-and-gemini\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:10,154 - courlan.urlstore - WARNING - Discarding URL: /blog-post/image-extraction-with-langchain-and-gemini\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:10,154 - courlan.urlstore - WARNING - Discarding URL: /blog-post/image-extraction-with-langchain-and-gemini\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:11,335 - courlan.urlstore - WARNING - Discarding URL: /blog-post/the-basics-of-entity-analysis-beyond-just-identifying-names\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:11,335 - courlan.urlstore - WARNING - Discarding URL: /blog-post/the-basics-of-entity-analysis-beyond-just-identifying-names\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:11,335 - courlan.urlstore - WARNING - Discarding URL: /blog-post/the-basics-of-entity-analysis-beyond-just-identifying-names\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:12,491 - courlan.urlstore - WARNING - Discarding URL: /job-list/software-engineer-front-end\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:12,491 - courlan.urlstore - WARNING - Discarding URL: /job-list/software-engineer-front-end\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:12,491 - courlan.urlstore - WARNING - Discarding URL: /job-list/software-engineer-front-end\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:13,629 - courlan.urlstore - WARNING - Discarding URL: /job-list/open-application---create-your-own-dream-job\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:13,629 - courlan.urlstore - WARNING - Discarding URL: /job-list/open-application---create-your-own-dream-job\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:13,629 - courlan.urlstore - WARNING - Discarding URL: /job-list/open-application---create-your-own-dream-job\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:14,781 - courlan.urlstore - WARNING - Discarding URL: /job-list/junior-ai-researcher\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:14,781 - courlan.urlstore - WARNING - Discarding URL: /job-list/junior-ai-researcher\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:14,781 - courlan.urlstore - WARNING - Discarding URL: /job-list/junior-ai-researcher\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:15,941 - courlan.urlstore - WARNING - Discarding URL: /job-list/internship\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:15,941 - courlan.urlstore - WARNING - Discarding URL: /job-list/internship\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:15,941 - courlan.urlstore - WARNING - Discarding URL: /job-list/internship\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:17,094 - courlan.urlstore - WARNING - Discarding URL: /offerings/chat-ui\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:17,094 - courlan.urlstore - WARNING - Discarding URL: /offerings/chat-ui\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:17,094 - courlan.urlstore - WARNING - Discarding URL: /offerings/chat-ui\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:18,895 - courlan.urlstore - WARNING - Discarding URL: /offerings/advice\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:18,895 - courlan.urlstore - WARNING - Discarding URL: /offerings/advice\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:18,895 - courlan.urlstore - WARNING - Discarding URL: /offerings/advice\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:20,039 - courlan.urlstore - WARNING - Discarding URL: /offerings/projects\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:20,039 - courlan.urlstore - WARNING - Discarding URL: /offerings/projects\u001b[0m\n",
      "\u001b[33m2024-08-20 16:47:20,039 - courlan.urlstore - WARNING - Discarding URL: /offerings/projects\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added links to confirm for base URL: https://www.vectrix.ai/\n",
      "Set/updated scrape status for base URL: https://www.vectrix.ai/\n"
     ]
    }
   ],
   "source": [
    "from vectrix.importers import WebScraper\n",
    "\n",
    "scraper = WebScraper(\"https://www.vectrix.ai/\", \"Vectrix\")\n",
    "all_links = scraper.get_all_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_pages = scraper.get_all_pages(all_links)\n",
    "len(web_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectrix.importers import chunk_content\n",
    "chunked_webpages = chunk_content(web_pages)\n",
    "\n",
    "\n",
    "print(f\"Before chunking we had {len(web_pages)} and after chunking {len(chunked_webpages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the data to a VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.add_documents(chunked_webpages, \"Vectrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db.similarity_search(\"Vectrix\", \"When was Vectrix founded?\", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From OneDrive ‚òÅ\n",
    "We can also connect with a OneDrive folder and download the information from there. After this process we can add the information to our Vector store in the same way. This way we can extract various kinds of documents like PDF, World, PowerPoint ...\n",
    "\n",
    "Downloads files from OneDrive, processes them with Unstructured, and stores in Weaviate.\n",
    "\n",
    "Steps:\n",
    "1. Connect to OneDrive using Azure credentials\n",
    "2. List and download all files\n",
    "3. Process files with Unstructured importer\n",
    "4. Add processed documents to Weaviate vector store\n",
    "5. Close OneDrive connection\n",
    "\n",
    "Requirements:\n",
    "- vectrix library (OneDrive connector, Unstructured importer)\n",
    "- Weaviate client\n",
    "- Azure credentials as environment variables (AZURE_CLIENT_ID, AZURE_CLIENT_SECRET)\n",
    "\n",
    "Note: Close OneDrive connection before uploading to vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectrix.connectors.onedrive import OneDrive\n",
    "from vectrix.importers.unstructured import Unstructured\n",
    "\n",
    "drive = OneDrive(azure_client_id = os.environ.get('AZURE_CLIENT_ID'), azure_client_secret = os.environ.get('AZURE_CLIENT_SECRET'))\n",
    "\n",
    "drive.list_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all files in the drive, returns a list of downloaded files\n",
    "downloaded_files = drive.download_files()\n",
    "print(downloaded_files)\n",
    "\n",
    "# Process teh documents using unstructured (we can't load RAW PDF files into a Vector store)\n",
    "importer = Unstructured()\n",
    "documents = importer.process_files(downloaded_files)\n",
    "\n",
    "# Add the documents to the Vector store\n",
    "weaviate = Weaviate()\n",
    "weaviate.set_colleciton('Vectrix')\n",
    "weaviate.add_data(documents)\n",
    "\n",
    "# Close the drive, do this BEFORE you want to upload the files to a vector store\n",
    "drive.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
