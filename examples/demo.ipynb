{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import vectrix\n",
    "import os\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectrix Demo üë®üèª‚Äçüíª\n",
    "This notebook demonstrates the functions for importing data from various sources. \n",
    "Loading it into a VectorStore, and then using it to answer questions with a Retrieval Augemented Reasoning  ü¶úüîó LangGraph.\n",
    "\n",
    "## Importing Data\n",
    "### 1. From a URL üîó\n",
    "\n",
    "**Web Crawling and Data Extraction Example**\n",
    "\n",
    "This cell demonstrates the process of crawling a website, chunking the extracted content, and performing Named Entity Recognition (NER) using the Vectrix library.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Web Crawling**: \n",
    "   - Uses `vectrix.Crawler` to extract pages from \"https://vectrix.ai\"\n",
    "   - Limits the crawl to a maximum of 20 pages\n",
    "\n",
    "2. **Content Chunking**:\n",
    "   - Utilizes `vectrix.Webchunker` to break down the extracted content\n",
    "   - Chunks are created with a size of 500 characters and an overlap of 50 characters\n",
    "\n",
    "3. **Named Entity Recognition**:\n",
    "   - Employs `vectrix.Extract` with the 'Replicate' model\n",
    "   - Uses the 'meta/meta-llama-3-70b-instruct' model for entity extraction\n",
    "\n",
    "This example showcases a typical workflow for web data extraction and processing using the Vectrix library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data from a URL\n",
    "\n",
    "crawler = vectrix.Crawler(\"https://vectrix.ai\", max_pages=20)\n",
    "site_pages = crawler.extract()\n",
    "\n",
    "print(f\"Extracted {len(site_pages)} pages\")\n",
    "\n",
    "# Chunk the data\n",
    "chunker = vectrix.Webchunker(site_pages)\n",
    "chunks = chunker.chunk_content(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# Extract NER info (Named Entities)\n",
    "extractor = vectrix.Extract('Replicate', 'meta/meta-llama-3-70b-instruct')\n",
    "results = extractor.extract(chunks)\n",
    "\n",
    "print(\"Example CHUNK: \")\n",
    "print(chunks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Data to Vector Store using Weaviate\n",
    "\n",
    "This code demonstrates how to add extracted web data to a Weaviate vector store using the Vectrix library.\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. **Import and Instantiate Weaviate**:\n",
    "   - Import necessary modules from Vectrix\n",
    "   - Create a Weaviate instance\n",
    "\n",
    "2. **Create a Collection**:\n",
    "   - Set up a new collection named 'Vectrix'\n",
    "   - Use a local Ollama model for embeddings\n",
    "   - Specify the model name and URL\n",
    "\n",
    "3. **Prepare Data for Vectorization**:\n",
    "   - Create `VectorDocument` objects for each extracted result\n",
    "   - Include metadata such as title, URL, content, type, and NER information\n",
    "\n",
    "4. **Add Data to Vector Store**:\n",
    "   - Use `weaviate.add_data()` to insert the prepared documents\n",
    "\n",
    "5. **Perform a Test Query**:\n",
    "   - Get a retriever from the Weaviate instance\n",
    "   - Execute a sample query to verify functionality\n",
    "\n",
    "**Note:**\n",
    "- You can remove a collection using `weaviate.remove_collection(\"Vectrix\")`\n",
    "- This example uses a local Ollama model, but you can use any compatible embedding model\n",
    "\n",
    "This code showcases the process of storing and querying vectorized web data using Weaviate and Vectrix, enabling efficient semantic search and retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectrix.db import Weaviate\n",
    "from vectrix.models.documents import VectorDocument\n",
    "\n",
    "# Instantiation of the Vector store\n",
    "weaviate = Weaviate()\n",
    "\n",
    "weaviate.remove_collection(\"Vectrix\")\n",
    "\n",
    "# Create a new collection to store the documents (you can also append them to an existsing one)\n",
    "weaviate.create_collection(name='Vectrix', \n",
    "                           embedding_model='Ollama', # For this example we use a local Ollama model, but you can use any other model \n",
    "                           model_name=\"mxbai-embed-large:335m\",\n",
    "                           model_url=\"http://host.docker.internal:11434\") # Ollama URL\n",
    "\n",
    "data_to_vectorize = []\n",
    "\n",
    "\n",
    "# Now let's add our downloaded webpages to the Vector store.\n",
    "for result in results:\n",
    "    data_to_vectorize.append(\n",
    "        VectorDocument(\n",
    "            title=result.metadata[\"title\"],\n",
    "            url=result.metadata[\"source\"],\n",
    "            content=result.page_content,\n",
    "            type=\"webpage\",\n",
    "            NER=str(result.metadata[\"NER\"]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "weaviate.add_data(data_to_vectorize)\n",
    "\n",
    "\n",
    "# Perform a quick search to see if it works\n",
    "retriever = weaviate.get_retriever()\n",
    "retriever.invoke('Who are the Vectrix founders ?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From OneDrive ‚òÅ\n",
    "We can also connect with a OneDrive folder and download the information from there. After this process we can add the information to our Vector store in the same way. This way we can extract various kinds of documents like PDF, World, PowerPoint ...\n",
    "\n",
    "Downloads files from OneDrive, processes them with Unstructured, and stores in Weaviate.\n",
    "\n",
    "Steps:\n",
    "1. Connect to OneDrive using Azure credentials\n",
    "2. List and download all files\n",
    "3. Process files with Unstructured importer\n",
    "4. Add processed documents to Weaviate vector store\n",
    "5. Close OneDrive connection\n",
    "\n",
    "Requirements:\n",
    "- vectrix library (OneDrive connector, Unstructured importer)\n",
    "- Weaviate client\n",
    "- Azure credentials as environment variables (AZURE_CLIENT_ID, AZURE_CLIENT_SECRET)\n",
    "\n",
    "Note: Close OneDrive connection before uploading to vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectrix.connectors.onedrive import OneDrive\n",
    "from vectrix.importers.unstructured import Unstructured\n",
    "\n",
    "drive = OneDrive(azure_client_id = os.environ.get('AZURE_CLIENT_ID'), azure_client_secret = os.environ.get('AZURE_CLIENT_SECRET'))\n",
    "\n",
    "drive.list_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all files in the drive, returns a list of downloaded files\n",
    "downloaded_files = drive.download_files()\n",
    "print(downloaded_files)\n",
    "\n",
    "# Process teh documents using unstructured (we can't load RAW PDF files into a Vector store)\n",
    "importer = Unstructured()\n",
    "documents = importer.process_files(downloaded_files)\n",
    "\n",
    "# Add the documents to the Vector store\n",
    "weaviate = Weaviate()\n",
    "weaviate.set_colleciton('Vectrix')\n",
    "weaviate.add_data(documents)\n",
    "\n",
    "# Close the drive, do this BEFORE you want to upload the files to a vector store\n",
    "drive.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
