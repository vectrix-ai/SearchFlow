{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRAG-ReAct: Intelligent Retrieval & Reasoning\n",
    "## Robust Retrieval-Augmented Generation with Step-by-Step Reasoning\n",
    "\n",
    "Welcome to this Jupyter notebook exploring cutting-edge techniques in artificial intelligence. We'll be diving into a powerful combination of two advanced approaches:\n",
    "\n",
    "1. **Corrective Retrieval Augmented Generation (CRAG)**\n",
    "2. **Reasoning+Acting (ReAct)**\n",
    "\n",
    "![Local image](./assets/RAG.png \"Flow Diagram\")\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "<small>\n",
    "\n",
    "**SOURCES:**\n",
    "\n",
    "[The paper](https://arxiv.org/abs/2210.03629) introduces ReAct, an approach that combines reasoning traces and task-specific actions in large language models (LLMs) to enhance their problem-solving capabilities. ReAct demonstrates improved performance, interpretability, and trustworthiness across various tasks, including question answering, fact verification, and interactive decision making, by allowing LLMs to generate reasoning steps and actions in an interleaved manner while interacting with external sources.\n",
    "\n",
    "\n",
    "[The Corrective Retrieval Augmented Generation (CRAG)](https://arxiv.org/abs/2401.15884) is a proposed approach to improve the robustness of language model generation by incorporating a lightweight retrieval evaluator, large-scale web searches, and a decompose-then-recompose algorithm for retrieved documents. CRAG aims to enhance the performance of RAG-based approaches by assessing retrieval quality, augmenting results with web searches when necessary, and selectively focusing on key information while filtering out irrelevant content.\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you're running this in an IPython environment that supports top-level await\n",
    "# You might need to run '%autoawait asyncio' at the start of your notebook if it's not enabled by default\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from paginx.graphs.vectrix import RAGWorkflowGraph\n",
    "from langchain_core.messages import HumanMessage\n",
    "from IPython.display import Image, display\n",
    "from paginx.db.postgresql import PostgresSaver, BaseCheckpointSaver\n",
    "import os\n",
    "from langsmith import Client\n",
    "from psycopg_pool import AsyncConnectionPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Enable langsmith tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "DB_URI = os.getenv(\"DB_URI\")\n",
    "\n",
    "# Create the pool without opening it\n",
    "pool = AsyncConnectionPool(\n",
    "    conninfo=DB_URI,\n",
    "    max_size=20,\n",
    "    open=False  # This prevents the pool from opening in the constructor\n",
    ")\n",
    "\n",
    "# Explicitly open the pool\n",
    "await pool.open()\n",
    "\n",
    "checkpointer = PostgresSaver(async_connection=pool)\n",
    "await checkpointer.acreate_tables(pool)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"999999999\"}}\n",
    "\n",
    "demo_graph = RAGWorkflowGraph(DB_URI=DB_URI)\n",
    "graph = demo_graph.create_graph(checkpointer=checkpointer)\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "input = HumanMessage(content=\"How's the weather in Antwerp ?\")\n",
    "\n",
    "run_id = \"\"\n",
    "langgraph_node = \"\"\n",
    "\n",
    "async for event in graph.astream_events({\"question\": input}, version=\"v1\", config=config):\n",
    "    run_id = event['run_id']\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        if langgraph_node != event['metadata']['langgraph_triggers']:\n",
    "            langgraph_node = event['metadata']['langgraph_triggers']\n",
    "            print(f\"Answering Step: {langgraph_node[0].split(':')[-1]}\")\n",
    "        if event['metadata']['langgraph_node'] == \"generate_response\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                print(content, end=\"|\")\n",
    "    # Print the documents used to generate the answer, if any\n",
    "    if kind == \"on_chain_end\":\n",
    "        if event[\"name\"] == \"generate_response\":\n",
    "            sources = []\n",
    "            for doc in event[\"data\"][\"input\"][\"documents\"]:\n",
    "                sources.append(doc.dict())\n",
    "            print(sources)\n",
    "\n",
    "\n",
    "client = Client()\n",
    "run = client.read_run(run_id)\n",
    "print('\\n\\n', run.url)\n",
    "\n",
    "# Don't forget to close the pool when you're done\n",
    "# You can run this in a separate cell when you're finished\n",
    "await pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = OllamaFunctions(model=\"llama3-groq-tool-use:8b-q8_0\", format=\"json\")\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "grade_chain =  grade_prompt | structured_llm_grader\n",
    "\n",
    "response = grade_chain.ainvoke({\"document\": \"25 degrees currently in NYC\", \"question\": \"How's the weather in Antwerp ?\"})\n",
    "answer = await response\n",
    "print(answer.binary_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.output_parsers.openai_tools import JsonOutputKeyToolsParser\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3-groq-tool-use:8b-q8_0\",\n",
    "    temperature=0,)\n",
    "\n",
    "llm = llm.bind_tools([GradeDocuments])\n",
    "\n",
    "#parser = JsonOutputToolsParser()\n",
    "#parser = JsonOutputKeyToolsParser(key_name=\"GradeDocuments\")\n",
    "parser = PydanticToolsParser(tools=[GradeDocuments])\n",
    "\n",
    "\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "grade_chain =  grade_prompt | llm | parser\n",
    "\n",
    "response = grade_chain.ainvoke({\"document\": \"25 degrees currently in NYC\", \"question\": \"How's the weather in Antwerp ?\"})\n",
    "answer = await response\n",
    "answer[0].binary_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
