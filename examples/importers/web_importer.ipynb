{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import paginx\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Page Indexing and Vectorization ðŸ‘€\n",
    "\n",
    "This Jupyter notebook contains a script that performs indexing and vectorization of web page contents. The primary purpose of this script is to crawl through a specified web page, extract the textual contents, and subsequently store these contents as vector objects in a database.\n",
    "\n",
    "The vectorized information can then be utilized in a Retrieval-Augmented Generation (RAG) flow to answer questions using a Language Model (LLM). This process enables the creation of a more context-aware and responsive system, capable of providing detailed responses based on the indexed and vectorized information from the web page.\n",
    "\n",
    "The notebook is structured in a step-by-step manner, guiding you through the process of web page crawling, text extraction, vectorization, and storage in a database. Each step is accompanied by detailed explanations and code snippets to provide a comprehensive understanding of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crawler and Content Extractor\n",
    "\n",
    "This code implements a web crawler and content extractor that:\n",
    "\n",
    "1. Extracts URLs from the given HTML content, filtering for the same domain and validating the URLs. âœ…\n",
    "2. Crawls a website starting from a given URL, iteratively processing and extracting links from each page. âœ…\n",
    "3. Returns a mist of HTML documents extracted from the website âœ…\n",
    "\n",
    "The code displays the source URL of each processed page and the total number of pages in the extracted content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  1.76it/s]\n",
      "Fetching pages: 100%|##########| 6/6 [00:02<00:00,  2.04it/s]\n",
      "Fetching pages: 100%|##########| 10/10 [00:04<00:00,  2.39it/s]\n"
     ]
    }
   ],
   "source": [
    "crawler = paginx.Crawler(\"https://vectrix.ai\")\n",
    "site_pages = crawler.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(site_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Job Description', 'author': None, 'hostname': None, 'date': None, 'fingerprint': '395059d9a3b4a234', 'id': None, 'license': None, 'comments': None, 'raw_text': \"At Vectrix, we're passionate about cultivating a space where learning, innovation, and personal growth go hand-in-hand. We're on the lookout for diverse, enthusiastic talent who share our vision and are eager to make a tangible impact in the world of AI. Embark on a journey with Vectrix, where the exciting world of AI awaits you. With a spectrum of specialties and niches, there's a place for every interest. We value your ideas and initiative. Feel free to suggest topics you're passionate about or consider conducting your thesis under the mentorship of our seasoned team. What Vectrix is looking for: You're a student in Computer Science, Data Science, or a related field, looking for an energizing environment to work with the latest in data and machine learning tech. You've got a knack for analytics, are comfortable with mathematical concepts, and are intrigued by research. Your toolkit includes skills in Python, SQL, and other statistical analysis tools. Fluent in English, both written and spoken. Any experience with TensorFlow, PyTorch, or cloud deployment is a bonus. You're enthusiastic about starting your career on a strong note, and Vectrix is the perfect springboard for that! Please note: Our internships at Vectrix are designed to complement your academic journey. They are unpaid, as we focus on enriching your skills and career path. An internship agreement through your educational institution is essential to join us.\", 'text': \"At Vectrix, we're passionate about cultivating a space where learning, innovation, and personal growth go hand-in-hand. We're on the lookout for diverse, enthusiastic talent who share our vision and are eager to make a tangible impact in the world of AI.\\nEmbark on a journey with Vectrix, where the exciting world of AI awaits you. With a spectrum of specialties and niches, there's a place for every interest.\\nWe value your ideas and initiative. Feel free to suggest topics you're passionate about or consider conducting your thesis under the mentorship of our seasoned team.\\nWhat Vectrix is looking for:\\nYou're a student in Computer Science, Data Science, or a related field, looking for an energizing environment to work with the latest in data and machine learning tech.\\n-\\nYou've got a knack for analytics, are comfortable with mathematical concepts, and are intrigued by research.\\n-\\nYour toolkit includes skills in Python, SQL, and other statistical analysis tools.\\n-\\nFluent in English, both written and spoken.\\n-\\nAny experience with TensorFlow, PyTorch, or cloud deployment is a bonus.\\n-\\nYou're enthusiastic about starting your career on a strong note, and Vectrix is the perfect springboard for that!\\n-\\nPlease note: Our internships at Vectrix are designed to complement your academic journey. They are unpaid, as we focus on enriching your skills and career path. An internship agreement through your educational institution is essential to join us.\", 'language': None, 'image': None, 'pagetype': None, 'filedate': '2024-07-23', 'source': None, 'source-hostname': None, 'excerpt': None, 'categories': '', 'tags': ''}\n"
     ]
    }
   ],
   "source": [
    "print(site_pages[8].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Chunking\n",
    "In this step we will split all the extracted web pages into logical chunks. \n",
    "\n",
    "âž¡ï¸ We will use the [trafilatura](https://trafilatura.readthedocs.io/en/latest/) library to extract the main content of the web pages. It will return the main content of the page, the title, and the meta description.\n",
    "\n",
    "âž¡ï¸ We will pipe this to another splitter to further cut the sections into smaller chunks if they are too large. For this we use Langchains \n",
    "\n",
    "âž¡ï¸  Also we will attach an LLM to the chain to ignore chunks that are not relevant, for example: navigation bars, footers, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking and metadata extraction\n",
    "Using the functions below we extract the medata and devide the text into chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = paginx.Webchunker(site_pages)\n",
    "chunks = chunker.chunk_content(chunk_size=500, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': None, 'metadata': {'source': 'https://vectrix.ai/offerings/chat-ui', 'title': 'Vectrix', 'language': 'en'}, 'page_content': 'Vectrix Chat Vectrix Chat is an internal tool that allows you to interact with various Large Language Models (LLMs) and easily switch between them. It also allows you to create your own custom models and assistants. \\u200d Key Features Multiple LLM Support: Vectrix Chat integrates with popular LLMs like Google Gemini, Anthropic Claude, and OpenAI GPT-4, allowing you to quickly test and compare different models. Custom Model Integration: You can easily connect and interact with your own custom models that run locally or in the cloud. Prompt Storage & Reuse: Create, store, and reuse prompts for commonly used tasks, saving you time and effort. Assistant Creation: Build custom assistants with specific instructions and files. Think of these assistants as dedicated GPT instances, tailored to your specific needs. File Integration: Attach files to your chats or assistants, allowing the LLMs to access and process relevant information. Chat History: All your chats are saved in a searchable history, making it easy to revisit and refer back to past conversations. Using Vectrix Chat 1. Select a Model: From the dropdown list, choose the LLM you want to use for your current task. 2.Input Your Prompt: Enter your prompt in the text area provided.Attach Files (Optional): If you need to provide additional context or data, upload relevant files. 3. Create an Assistant (Optional): If you want to build a dedicated AI assistant, click the \"+ New Assistant\" button and provide the necessary details, such as a name, description, model, prompt, and files. 4. Interact with the LLM: The LLM will process your prompt and provide a response. You can continue to interact with the LLM and refine your prompt as needed. \\u200d Benefits Streamlined AI Integration: Vectrix Chat simplifies the process of working with LLMs, providing a centralized platform for testing, exploring, and leveraging the power of AI. Enhanced Efficiency: Prompt storage and reuse, as well as custom assistant creation, boost your productivity by eliminating repetitive tasks. Data Control: Maintain control over your data by integrating your own models and files. Getting Started If you need assistance using Vectrix Chat, please contact the AI team. We\\'re happy to help! Licence Vectrix Chat is based on the open-source package: https://github.com/mckaywrigley/chatbot-ui and is covered by the MIT Licence.', 'type': 'Document'}\n"
     ]
    }
   ],
   "source": [
    "print(chunks[18].dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": null,\n",
      "  \"metadata\": {\n",
      "    \"source\": \"https://vectrix.ai/offerings/chat-ui\",\n",
      "    \"title\": \"Vectrix\",\n",
      "    \"language\": \"en\"\n",
      "  },\n",
      "  \"page_content\": \"Vectrix Chat Vectrix Chat is an internal tool that allows you to interact with various Large Language Models (LLMs) and easily switch between them. It also allows you to create your own custom models and assistants. \\u200d Key Features Multiple LLM Support: Vectrix Chat integrates with popular LLMs like Google Gemini, Anthropic Claude, and OpenAI GPT-4, allowing you to quickly test and compare different models. Custom Model Integration: You can easily connect and interact with your own custom models that run locally or in the cloud. Prompt Storage & Reuse: Create, store, and reuse prompts for commonly used tasks, saving you time and effort. Assistant Creation: Build custom assistants with specific instructions and files. Think of these assistants as dedicated GPT instances, tailored to your specific needs. File Integration: Attach files to your chats or assistants, allowing the LLMs to access and process relevant information. Chat History: All your chats are saved in a searchable history, making it easy to revisit and refer back to past conversations. Using Vectrix Chat 1. Select a Model: From the dropdown list, choose the LLM you want to use for your current task. 2.Input Your Prompt: Enter your prompt in the text area provided.Attach Files (Optional): If you need to provide additional context or data, upload relevant files. 3. Create an Assistant (Optional): If you want to build a dedicated AI assistant, click the \\\"+ New Assistant\\\" button and provide the necessary details, such as a name, description, model, prompt, and files. 4. Interact with the LLM: The LLM will process your prompt and provide a response. You can continue to interact with the LLM and refine your prompt as needed. \\u200d Benefits Streamlined AI Integration: Vectrix Chat simplifies the process of working with LLMs, providing a centralized platform for testing, exploring, and leveraging the power of AI. Enhanced Efficiency: Prompt storage and reuse, as well as custom assistant creation, boost your productivity by eliminating repetitive tasks. Data Control: Maintain control over your data by integrating your own models and files. Getting Started If you need assistance using Vectrix Chat, please contact the AI team. We're happy to help! Licence Vectrix Chat is based on the open-source package: https://github.com/mckaywrigley/chatbot-ui and is covered by the MIT Licence.\",\n",
      "  \"type\": \"Document\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(chunks[18].json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER Extraction Pipeline\n",
    "Here we will use langchain and and LLM to extract the Named Entities from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting entities: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:15<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "extractor = paginx.Extract('Replicate', 'meta/meta-llama-3-70b-instruct')\n",
    "results = extractor.extract(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://vectrix.ai/about-us', 'title': 'Vectrix - About Us', 'description': 'Established in 2023, Vectrix began as a small but ambitious team, aware of the growing demand for inventive, impactful solutions in the fast-paced digital era. Our expertise lies in blending creativity with generative AI technology to help businesses excel.', 'language': 'en', 'uuid': '0a7c7c18-dacc-41d3-8385-2dffda666822', 'NER': {'entity_list': [{'entity_type': 'person', 'entity_name': 'Ben Selleslagh'}, {'entity_type': 'person', 'entity_name': 'Dimitri Allaert'}, {'entity_type': 'organization', 'entity_name': 'Vectrix'}, {'entity_type': 'organization', 'entity_name': 'BUFFL'}, {'entity_type': 'technology', 'entity_name': 'Google Cloud'}], 'language': 'English', 'category': 'Company Profile'}}\n"
     ]
    }
   ],
   "source": [
    "print(results[4].dict()['metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used:  275.625 MB\n"
     ]
    }
   ],
   "source": [
    "# Show the memory usage of this notebook\n",
    "import os\n",
    "import psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Memory used: \", process.memory_info().rss / 1024 ** 2, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the results in a Chroma DB Object\n",
    "Chroma is a fast and easy to use Vector database that can be used to load the retrieved content in memory and use a RAG-chain to retrieve the information. You can also persist the data in Chroma to disk for later use. We also use the langchain implementation to store the data in Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "chroma = paginx.Chroma(CohereEmbeddings())\n",
    "vector_db = chroma.create_db(results, os.getenv('CHROMA_DB_LOCATION'))\n",
    "\n",
    "# Let's perform a search and see of this works ...\n",
    "search_results = vector_db.similarity_search('Who are the founders of Vectrix ?, 3')\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = vector_db.similarity_search_with_score('Who are the founders of Vectrix ?, 3')\n",
    "\n",
    "for result in search_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the results in PostgreSQL (pgvector)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Pull the postgres image, install the pgvector extension and run the container\n",
    "!docker pull ankane/pgvector\n",
    "!docker run -d --name paginx -e POSTGRES_PASSWORD=mysecretpassword -p 5432:5432 -e PG_EXTENSIONS=\"pgvector\" ankane/pgvector"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Connect to the default database (usually 'postgres')\n",
    "default_engine = create_engine('postgresql://postgres:mysecretpassword@localhost/postgres')\n",
    "\n",
    "# Create a new database named paginx if it doesn't exist\n",
    "with default_engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT 1 FROM pg_database WHERE datname = 'paginx'\"))\n",
    "    if not result.scalar():\n",
    "        connection.execute(text(\"COMMIT\"))  # Commit any open transactions\n",
    "        connection.execute(text(\"CREATE DATABASE paginx\"))\n",
    "\n",
    "# Connect to the newly created database\n",
    "paginx_engine = create_engine('postgresql://postgres:mysecretpassword@localhost/paginx')\n",
    "\n",
    "# Install the pgvector extension in the paginx database\n",
    "with paginx_engine.connect() as connection:\n",
    "    connection.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "connection = \"postgresql+psycopg://postgres:mysecretpassword@localhost/paginx\"\n",
    "collection_name = url\n",
    "embeddings = CohereEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorstore.drop_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.add_documents(results, ids=[result.metadata[\"uuid\"] for result in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (vectorstore.similarity_search(\"When is the company founded ? \", k=3)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the result in a Weaviate (cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Vector store and check that all the required modules are installed\n",
    "\n",
    "Download the Docker compose file if needed\n",
    "```bash\n",
    "curl -o docker-compose.yml \"https://configuration.weaviate.io/v2/docker-compose/docker-compose.yml?cohere_key_approval=yes&generative_anyscale=false&generative_aws=false&generative_cohere=false&generative_mistral=false&generative_octoai=false&generative_ollama=false&generative_openai=false&generative_palm=false&media_type=text&modules=modules&ner_module=false&qna_module=false&ref2vec_centroid=false&reranker_cohere=true&reranker_cohere_key_approval=yes&reranker_transformers=false&runtime=docker-compose&spellcheck_module=true&spellcheck_module_model=pyspellchecker-en&sum_module=false&text_module=text2vec-cohere&weaviate_version=v1.25.4&weaviate_volume=named-volume\"\n",
    "```\n",
    "\n",
    "Make sure to set the persistent directory to the correct value:\n",
    "```bash\n",
    "    volumes:\n",
    "    - ~/weaviate_data:/var/lib/weaviate\n",
    "```\n",
    "\n",
    "Also configure the Cohere API key:\n",
    "```bash\n",
    "environment:\n",
    "      SPELLCHECK_INFERENCE_API: 'http://text-spellcheck:8080'\n",
    "      COHERE_APIKEY: ***\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hostname': 'http://[::]:8080', 'modules': {'generative-ollama': {'documentationHref': 'https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion', 'name': 'Generative Search - Ollama'}, 'text-spellcheck': {'model': {'name': 'pyspellchecker'}}, 'text2vec-ollama': {'documentationHref': 'https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings', 'name': 'Ollama Module'}}, 'version': '1.26.0'}\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "meta_info = client.get_meta()\n",
    "print(meta_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weaviate.collections.collection.Collection at 0x314b43bc0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from weaviate.classes.config import Configure\n",
    "\n",
    "client.collections.create(\n",
    "    \"Vectrix\",\n",
    "    vectorizer_config=[\n",
    "        Configure.NamedVectors.text2vec_ollama(\n",
    "            name=\"content\",\n",
    "            source_properties=[\"title\", \"url\", \"NER\", \"content\", \"type\"],\n",
    "            model=\"mxbai-embed-large:335m\",\n",
    "            api_endpoint=\"http://host.docker.internal:11434\"\n",
    "            \n",
    "              # The model to use, e.g. \"nomic-embed-text\"\n",
    "        )\n",
    "    ],\n",
    "    # Additional parameters not shown\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectrix\n"
     ]
    }
   ],
   "source": [
    "for collection in client.collections.list_all():\n",
    "    print(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': None, 'metadata': {'source': 'https://vectrix.ai/career', 'title': 'Vectrix - Career', 'description': 'Discover exciting career opportunities with us and be part of a dynamic team that is revolutionising the industry.', 'language': 'en', 'uuid': 'd791d5b4-07a1-48b6-b701-0126e11fad66', 'NER': {'entity_list': [{'entity_type': 'organization', 'entity_name': 'Vectrix'}], 'language': 'English', 'category': 'Artificial Intelligence'}, 'type': 'webpage'}, 'page_content': \"At Vectrix, we specialize in training and validating Small Language Models (SLMs) on business data. Choose our platform for a DIY approach or collaborate with our experts. We make complex tech simple with tailored AI solutions. Ready to transform your work? Let's innovate together!\", 'type': 'Document'}\n"
     ]
    }
   ],
   "source": [
    "print(results[3].dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.collections.get(\"Vectrix\")\n",
    "\n",
    "with collection.batch.dynamic() as batch:\n",
    "    for result in results:\n",
    "        result.metadata[\"type\"] = \"webpage\"\n",
    "        weaviate_object = {\n",
    "            \"title\": result.dict()[\"metadata\"][\"title\"],\n",
    "            \"url\": result.dict()[\"metadata\"][\"source\"],\n",
    "            \"NER\": result.dict()[\"metadata\"][\"NER\"],\n",
    "            \"content\": result.dict()[\"page_content\"],\n",
    "            \"type\": \"webpage\",\n",
    "        }\n",
    "\n",
    "        batch.add_object(\n",
    "            properties=weaviate_object,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing a (hybrid) search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectrix - About Us\n",
      "Vectrix - Contact Us\n",
      "Vectrix\n"
     ]
    }
   ],
   "source": [
    "collection = client.collections.get(\"Vectrix\")\n",
    "\n",
    "response = collection.query.near_text(\n",
    "    query=\"Who are the founders of Vectrix ?\",  # The model provider integration will automatically vectorize the query\n",
    "    limit=3,\n",
    "    \n",
    ")\n",
    "\n",
    "for obj in response.objects:\n",
    "    print(obj.properties[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectrix \n",
      "\n",
      "https://vectrix.ai/job-list/junior-ai-researcher \n",
      "\n",
      "webpage \n",
      "\n",
      "\n",
      "Hybrid (Result Set keyword,bm25) Document 83b611e3-6d7a-4e66-b400-6dfaa92f2bdf: original score 1.2208344, normalized score: 0.17300671 - \n",
      "Hybrid (Result Set vector,hybridVector) Document 83b611e3-6d7a-4e66-b400-6dfaa92f2bdf: original score 0.6553893, normalized score: 0.702733\n",
      "Vectrix \n",
      "\n",
      "https://vectrix.ai/job-list/software-engineer-front-end \n",
      "\n",
      "webpage \n",
      "\n",
      "\n",
      "Hybrid (Result Set keyword,bm25) Document fe6feb9b-4753-4846-bef1-7a91aa2044dc: original score 1.1256851, normalized score: 0.1542456 - \n",
      "Hybrid (Result Set vector,hybridVector) Document fe6feb9b-4753-4846-bef1-7a91aa2044dc: original score 0.6558575, normalized score: 0.70386404\n",
      "Vectrix - About Us \n",
      "\n",
      "https://vectrix.ai/about-us \n",
      "\n",
      "webpage \n",
      "\n",
      "\n",
      "Hybrid (Result Set keyword,bm25) Document f4377e91-9f84-435a-9bfe-29eb87d166ab: original score 0.53118104, normalized score: 0.037023842 - \n",
      "Hybrid (Result Set vector,hybridVector) Document f4377e91-9f84-435a-9bfe-29eb87d166ab: original score 0.6956521, normalized score: 0.8\n"
     ]
    }
   ],
   "source": [
    "from weaviate.classes.query import MetadataQuery\n",
    "\n",
    "vectrix = client.collections.get(\"Vectrix\")\n",
    "response = vectrix.query.hybrid(\n",
    "    query=\"Who are the founders of Vectrix ?\",\n",
    "    alpha=0.8,\n",
    "    return_metadata=MetadataQuery(explain_score=True),\n",
    "    limit=3,\n",
    "    query_properties=['content'],\n",
    ")\n",
    "\n",
    "for o in response.objects:\n",
    "    #print(o.properties)\n",
    "    print(o.properties['title'], '\\n')\n",
    "    print(o.properties['url'], '\\n')\n",
    "    print(o.properties['type'], '\\n')\n",
    "    print(o.metadata.explain_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove a collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.collections.delete(\"Vectrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
