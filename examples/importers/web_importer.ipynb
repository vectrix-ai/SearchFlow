{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Page Indexing and Vectorization üëÄ\n",
    "\n",
    "This Jupyter notebook contains a script that performs indexing and vectorization of web page contents. The primary purpose of this script is to crawl through a specified web page, extract the textual contents, and subsequently store these contents as vector objects in a database.\n",
    "\n",
    "The vectorized information can then be utilized in a Retrieval-Augmented Generation (RAG) flow to answer questions using a Language Model (LLM). This process enables the creation of a more context-aware and responsive system, capable of providing detailed responses based on the indexed and vectorized information from the web page.\n",
    "\n",
    "The notebook is structured in a step-by-step manner, guiding you through the process of web page crawling, text extraction, vectorization, and storage in a database. Each step is accompanied by detailed explanations and code snippets to provide a comprehensive understanding of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crawler and Content Extractor\n",
    "\n",
    "This code implements a web crawler and content extractor that:\n",
    "\n",
    "1. Extracts URLs from the given HTML content, filtering for the same domain and validating the URLs. ‚úÖ\n",
    "2. Crawls a website starting from a given URL, iteratively processing and extracting links from each page. ‚úÖ\n",
    "3. Returns a mist of HTML documents extracted from the website ‚úÖ\n",
    "\n",
    "The code displays the source URL of each processed page and the total number of pages in the extracted content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TO VISIT \n",
      " ['https://vectrix.ai/blog-post/your-ai-might-be-misleading-you-understanding-the-dual-nature-of-llm-outputs', 'https://vectrix.ai/contact-us', 'https://vectrix.ai/about-us', 'https://vectrix.ai/blog-post/understanding-large-and-small-language-models-key-differences-and-applications', 'https://vectrix.ai/platform', 'https://vectrix.ai/career', 'https://vectrix.ai/offerings', 'https://vectrix.ai/blog-post/google-deepminds-searchless-chess-engine---part-1', 'https://vectrix.ai/blog', 'https://vectrix.ai/blog-post/are-llm-benchmarks-and-leaderboards-just-marketing-tools'] \n",
      "\n",
      "KNOWN LINKS \n",
      " ['https://vectrix.ai/', 'https://vectrix.ai/blog-post/your-ai-might-be-misleading-you-understanding-the-dual-nature-of-llm-outputs', 'https://vectrix.ai/contact-us', 'https://vectrix.ai/about-us', 'https://vectrix.ai/blog-post/understanding-large-and-small-language-models-key-differences-and-applications', 'https://vectrix.ai/platform', 'https://vectrix.ai/career', 'https://vectrix.ai/offerings', 'https://vectrix.ai/blog-post/google-deepminds-searchless-chess-engine---part-1', 'https://vectrix.ai/blog', 'https://vectrix.ai/blog-post/are-llm-benchmarks-and-leaderboards-just-marketing-tools'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from trafilatura.spider import focused_crawler\n",
    "from trafilatura.settings import use_config\n",
    "\n",
    "newconfig = use_config(\"scraper_settings.cfg\")\n",
    "\n",
    "homepage = 'https://vectrix.ai'\n",
    "to_visit, known_links = focused_crawler(homepage, max_seen_urls=1, config=newconfig)\n",
    "\n",
    "print(\"TO VISIT \\n\", to_visit, \"\\n\")\n",
    "print(\"KNOWN LINKS \\n\", known_links, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:trafilatura.downloads:not a 200 response: 404 for URL https://vectrix.ai/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TO VISIT \n",
      " [] \n",
      "\n",
      "KNOWN LINKS \n",
      " ['https://vectrix.ai/', 'https://vectrix.ai/blog-post/your-ai-might-be-misleading-you-understanding-the-dual-nature-of-llm-outputs', 'https://vectrix.ai/contact-us', 'https://vectrix.ai/about-us', 'https://vectrix.ai/blog-post/understanding-large-and-small-language-models-key-differences-and-applications', 'https://vectrix.ai/platform', 'https://vectrix.ai/career', 'https://vectrix.ai/offerings', 'https://vectrix.ai/blog-post/google-deepminds-searchless-chess-engine---part-1', 'https://vectrix.ai/blog', 'https://vectrix.ai/blog-post/are-llm-benchmarks-and-leaderboards-just-marketing-tools', 'https://vectrix.ai/blog-post/advanced-applications-and-future-trends-in-entity-analysis', 'https://vectrix.ai/job-list/open-application---create-your-own-dream-job', 'https://vectrix.ai/job-list/internship', 'https://vectrix.ai/job-list/junior-ai-researcher', 'https://vectrix.ai/job-list/software-engineer-front-end', 'https://vectrix.ai/offerings/projects', 'https://vectrix.ai/offerings/advice', 'https://vectrix.ai/offerings/chat-ui', 'https://vectrix.ai/blog-post/the-basics-of-entity-analysis-beyond-just-identifying-names', 'https://vectrix.ai/blog-post/image-extraction-with-langchain-and-gemini'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "to_visit, known_links = focused_crawler(homepage, \n",
    "                                        max_seen_urls=1000, \n",
    "                                        max_known_urls=100000, \n",
    "                                        todo=to_visit, \n",
    "                                        known_links=known_links,\n",
    "                                        config=newconfig\n",
    "                                        )\n",
    "\n",
    "print(\"TO VISIT \\n\", to_visit, \"\\n\")\n",
    "print(\"KNOWN LINKS \\n\", known_links, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Listing all pages for domain: vectrix.ai\n",
      "\u001b[32m2024-08-12 17:06:10,122 - root - INFO - Listing all pages for domain: vectrix.ai\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa686a2d313645febee6aa7dce479793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing URLs: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trafilatura.downloads import add_to_compressed_dict, buffered_downloads, load_download_buffer\n",
    "from trafilatura import extract\n",
    "from langchain_core.documents import Document\n",
    "from tqdm.notebook import tqdm\n",
    "import json, os\n",
    "import hashlib\n",
    "from vectrix.db import DB\n",
    "# number of threads to use\n",
    "threads = 10\n",
    "\n",
    "\n",
    "db = DB(db_url=os.getenv('DB_URI'))\n",
    "\n",
    "results = []\n",
    "\n",
    "#  Extract the domain name from a URL\n",
    "def extract_domain(url):\n",
    "    from urllib.parse import urlparse\n",
    "    return urlparse(url).netloc\n",
    "\n",
    "domain_name = extract_domain(homepage)\n",
    "already_downloaded = db.list_documents(domain_name=\"vectrix.ai\")\n",
    "to_download = [url for url in known_links if url not in already_downloaded]\n",
    "\n",
    "# Add URLs to a compressed dictionary\n",
    "url_store = add_to_compressed_dict(to_download)\n",
    "\n",
    "\n",
    "# processing loop\n",
    "with tqdm(total=len(to_download), desc=\"Processing URLs\") as pbar:\n",
    "    while url_store.done is False:\n",
    "        bufferlist, url_store = load_download_buffer(url_store, sleep_time=0)\n",
    "        # process downloads\n",
    "        for url, result in buffered_downloads(bufferlist, threads):\n",
    "            # do something here\n",
    "            extracted_page = extract(result, output_format='json', include_links=True, with_metadata=True)\n",
    "\n",
    "\n",
    "            if extracted_page:\n",
    "                page_hash = hashlib.sha256(extracted_page.encode()).hexdigest()\n",
    "                db.add_document(\n",
    "                    url=url,\n",
    "                    page_hash=page_hash,\n",
    "                    domain_name=domain_name,\n",
    "                    storage_location=\"\",\n",
    "                    content=json.loads(extracted_page)\n",
    "                )\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "# Remove documents again\n",
    "# db.remove_documents(domain_name=\"vectrix.ai\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Chunking\n",
    "In this step we will split all the extracted web pages into logical chunks. \n",
    "\n",
    "‚û°Ô∏è We will use the [trafilatura](https://trafilatura.readthedocs.io/en/latest/) library to extract the main content of the web pages. It will return the main content of the page, the title, and the meta description.\n",
    "\n",
    "‚û°Ô∏è We will pipe this to another splitter to further cut the sections into smaller chunks if they are too large. For this we use Langchains \n",
    "\n",
    "‚û°Ô∏è  Also we will attach an LLM to the chain to ignore chunks that are not relevant, for example: navigation bars, footers, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking and metadata extraction\n",
    "Using the functions below we extract the medata and devide the text into chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "\u001b[33m2024-08-12 17:06:12,744 - langchain_community.utils.user_agent - WARNING - USER_AGENT environment variable not set, consider setting it to identify your requests.\u001b[0m\n",
      "INFO:root:Getting all documents for domain: vectrix.ai\n",
      "\u001b[32m2024-08-12 17:06:12,749 - root - INFO - Getting all documents for domain: vectrix.ai\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before chunking we had 21 and after chunking 21\n"
     ]
    }
   ],
   "source": [
    "from vectrix.importers import chunk_content\n",
    "webpages = db.get_documents(domain_name=\"vectrix.ai\")\n",
    "chunked_webpages = chunk_content(webpages)\n",
    "# \n",
    "\n",
    "\n",
    "print(f\"Before chunking we had {len(webpages)} and after chunking {len(chunked_webpages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the result in a Weaviate (cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Vector store and check that all the required modules are installed\n",
    "\n",
    "Download the Docker compose file if needed\n",
    "```bash\n",
    "curl -o docker-compose.yml \"https://configuration.weaviate.io/v2/docker-compose/docker-compose.yml?cohere_key_approval=yes&generative_anyscale=false&generative_aws=false&generative_cohere=false&generative_mistral=false&generative_octoai=false&generative_ollama=false&generative_openai=false&generative_palm=false&media_type=text&modules=modules&ner_module=false&qna_module=false&ref2vec_centroid=false&reranker_cohere=true&reranker_cohere_key_approval=yes&reranker_transformers=false&runtime=docker-compose&spellcheck_module=true&spellcheck_module_model=pyspellchecker-en&sum_module=false&text_module=text2vec-cohere&weaviate_version=v1.25.4&weaviate_volume=named-volume\"\n",
    "```\n",
    "\n",
    "Make sure to set the persistent directory to the correct value:\n",
    "```bash\n",
    "    volumes:\n",
    "    - ~/weaviate_data:/var/lib/weaviate\n",
    "```\n",
    "\n",
    "Also configure the Cohere API key:\n",
    "```bash\n",
    "environment:\n",
    "      SPELLCHECK_INFERENCE_API: 'http://text-spellcheck:8080'\n",
    "      COHERE_APIKEY: ***\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-12 17:07:41,337 - httpx - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration \"HTTP/1.1 404 Not Found\"\u001b[0m\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8080/v1/meta \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2024-08-12 17:07:41,356 - httpx - INFO - HTTP Request: GET http://localhost:8080/v1/meta \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "INFO:httpx:HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2024-08-12 17:07:41,414 - httpx - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723475261.425448  160548 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    }
   ],
   "source": [
    "from vectrix.db import Weaviate\n",
    "\n",
    "weaviate = Weaviate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/schema \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2024-08-12 17:08:02,928 - httpx - INFO - HTTP Request: POST http://localhost:8080/v1/schema \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "weaviate.create_collection(name='Vectrix', \n",
    "                           embedding_model='Ollama', \n",
    "                           model_name=\"mxbai-embed-large:335m\",\n",
    "                           model_url=\"http://host.docker.internal:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8080/v1/schema \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2024-08-12 17:07:54,706 - httpx - INFO - HTTP Request: GET http://localhost:8080/v1/schema \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vectrix', 'Elmos', 'Loop']\n"
     ]
    }
   ],
   "source": [
    "print(weaviate.list_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate.set_colleciton(name='Vectrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8080/v1/schema/Vectrix \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2024-08-12 17:08:30,601 - httpx - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Vectrix \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2024-08-12 17:08:30,603 - httpx - INFO - HTTP Request: GET http://localhost:8080/v1/nodes \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2024-08-12 17:08:31,611 - httpx - INFO - HTTP Request: GET http://localhost:8080/v1/nodes \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "weaviate.add_data(chunked_webpages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = weaviate.get_retriever()\n",
    "retriever.invoke('Who are the Vectrix founders ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: DELETE http://localhost:8080/v1/schema/Vectrix \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2024-08-12 17:07:59,740 - httpx - INFO - HTTP Request: DELETE http://localhost:8080/v1/schema/Vectrix \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "weaviate.remove_collection(\"Vectrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
