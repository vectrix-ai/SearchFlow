{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma fine-tuning ‚öôÔ∏èüë®üèª‚Äçüíª\n",
    "In this notebook we will use the Apple MLX framework to fine-tune the open source Gemma model created by Google.\n",
    "\n",
    "As an input for fine-tuning we will use the questions and responses generated by the larger llama3-70b model that was inferences via Replicate. This is a student teacher approach, we use a large model to generate the data and a smaller model to learn from it.\n",
    "\n",
    "The instruction for fine tuning this model are based on [this notebook](https://gist.github.com/alexweberk/635431b5c5773efd6d1755801020429f), I thank Alex for sharing this information üôè \n",
    "\n",
    " -> ***Currently I'm running this notebook on a 2023 M3 Pro with 36GB of RAM***\n",
    "\n",
    " We need to install the following packages\n",
    "- `transformers`\n",
    "- `mlx`\n",
    "- `torch`\n",
    "- `mlx_lm`\n",
    "\n",
    "-> It's recommended to use a package manager like [poetry](https://python-poetry.org/) to install the dependencies\n",
    "\n",
    "## The Model\n",
    "‚ö†Ô∏è We will the instruction tuned gemma-7b-it for this fine-tuning task ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import generate, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/GitHub/paginx/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd0e60db3de461995113def4aa702c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d315a64639e443a3881a0c2081a4b0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/620 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de23f8aa20b4d15a4d89b6fbd9ad0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75e2827e18e469fba932e89655c9ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09541e53dc2548db801a834183ba56c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6071dfe054d847b584228607ae5fc12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ce0645880e408792eae4140e2e21d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da22554be1a449e09b008f793e6345a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd4440227054ffb9ce7511f21913352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0bb542f0e9246f89be75be7cc15521b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15e1a4f10424ce7a78160532e3c686c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66022f77c72847d2ac0b3dbcd1f53331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load(\"google/gemma-1.1-7b-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is downloaded we can try to run a prompt to see that inferencing works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Why is the sky blue?\n",
      "\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "The sky is blue due to a phenomenon called **Rayleigh scattering**. \n",
      "\n",
      "* Sunlight is composed of all the colors of the rainbow, each with a specific wavelength.\n",
      "* When sunlight interacts with molecules in the atmosphere, such as nitrogen and oxygen, the molecules scatter the light.\n",
      "* Different wavelengths of light are scattered differently. \n",
      "* **Blue light has a shorter wavelength than other colors** and is scattered more efficiently by these molecules.\n",
      "* This means that more blue light is scattered in all directions, reaching our eyes and making the sky appear blue.\n",
      "==========\n",
      "Prompt: 0.853 tokens-per-sec\n",
      "Generation: 7.404 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "# Generating without adding a prompt template manually\n",
    "prompt = \"\"\"\n",
    "Why is the sky blue?\n",
    "\"\"\".strip()\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,  # Set to True to see the prompt and response\n",
    "    temp=0.0,\n",
    "    max_tokens=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset\n",
    "\n",
    "We need to create a dataset of JSONL files with the following structure:\n",
    "```json\n",
    "{\"text\": \"<bos><start_of_turn>user\\nWhat is the capital of France?<end_of_turn>\\n<start_of_turn>model\\nParis is the capital of France.<end_of_turn><eos>\"}\n",
    "```\n",
    "\n",
    "We need to create train.json and valid.json, having a 90/10 split between the two files. Let's create the dataset by querying the postgresql database we filled with in the site_crawl_processing example notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "connection = \"postgresql+psycopg://postgres:mysecretpassword@localhost/paginx\"\n",
    "\n",
    "engine = sqlalchemy.create_engine(connection)\n",
    "\n",
    "query = \"\"\"\n",
    "select \"document\" , cmetadata \n",
    "from langchain_pg_embedding lpe\n",
    "limit 1000\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'advertising', 'language': 'en', 'entity_list': [{'entity_name': 'Quiet 2', 'entity_type': 'product'}, {'entity_name': 'Loop', 'entity_type': 'brand'}]}\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0]['cmetadata']['NER'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
